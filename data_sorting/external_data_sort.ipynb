{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External sort algorithm on large data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, constants and setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "notebook_mode: int = int(\n",
    "    input(\n",
    "        \"\"\"\n",
    "    Select notebook mode:\n",
    "    1. Google Colab  2. Local\n",
    "    \"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "if notebook_mode == 1:\n",
    "    # Run on Colab.\n",
    "    INPUT_PATH: str = \"/content/drive/MyDrive/Ellinbank/video_observation/data/\"\n",
    "    SCRIPT_PATH: str = \"/content/drive/MyDrive/Ellinbank/video_observation/training_testing/data_labelling/\"\n",
    "    # OUTPUT_PATH: str = \"/content/drive/MyDrive/Ellinbank/video_observation/output/\"\n",
    "    OUTPUT_PATH: str = \"./out/\"\n",
    "    os.system(command=\"cp {}custom_model.py .\".format(SCRIPT_PATH))\n",
    "    os.system(command=\"cp {}inference.py .\".format(SCRIPT_PATH))\n",
    "    os.system(command=\"cp {}utils.py .\".format(SCRIPT_PATH))\n",
    "    os.system(command=\"cp {}operation.py .\".format(SCRIPT_PATH))\n",
    "elif notebook_mode == 2:\n",
    "    INPUT_PATH: str = \"../../../../data/\"\n",
    "    SCRIPT_PATH: str = \"./\"\n",
    "    OUTPUT_PATH: str = \"./out/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(command=\"rm -rf {}\".format(OUTPUT_PATH))\n",
    "try:\n",
    "    os.mkdir(path=OUTPUT_PATH)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MOS2E03230475']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "files: list[str] = os.listdir(path=INPUT_PATH)\n",
    "files = [f for f in files if f.endswith(\".zip\")]\n",
    "\n",
    "sensor_names: list[str] = [name.split(\"_\")[0] for name in files]\n",
    "print(sensor_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "window_size: int = 600  # 300: 10 seconds\n",
    "window_per_epoch: int = 200\n",
    "epoch: int = 1\n",
    "batch_size: int = 64\n",
    "# random.seed(715) # 715 looks good.\n",
    "random.seed(785)  # 785 makes \"other\" looks bad, otherwise is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the large data file and sort these partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import pandas.io.parsers.readers\n",
    "\n",
    "for f in files:\n",
    "    os.system(\"rm -rf {}{}_*.txt\".format(OUTPUT_PATH, f.split(\"_\")[0]))\n",
    "    # Path(\"{}{}.txt\".format(OUTPUT_PATH, f.split(\"_\")[0])).touch()\n",
    "    data_chunks: pandas.io.parsers.readers.TextFileReader = pd.read_csv(\n",
    "        filepath_or_buffer=\"{}/{}\".format(INPUT_PATH, f),\n",
    "        # nrows=14000,\n",
    "        chunksize=window_size * 1,\n",
    "    )\n",
    "    df_count: int = 1\n",
    "    raw_data: pd.DataFrame\n",
    "    for raw_data in data_chunks:\n",
    "        # Sort values based on timestamps.\n",
    "        raw_data.sort_values(\n",
    "            by=[\"timestamps\"],\n",
    "            ascending=True,\n",
    "            inplace=True,\n",
    "        )\n",
    "        raw_data = raw_data.reset_index(drop=True)\n",
    "        raw_data.to_csv(\n",
    "            path_or_buf=\"{}{}_{}.txt\".format(\n",
    "                OUTPUT_PATH,\n",
    "                f.split(\"_\")[0],\n",
    "                df_count,\n",
    "            ),\n",
    "            header=True,\n",
    "            index=False,\n",
    "        )\n",
    "        df_count += 1\n",
    "\n",
    "        # if df_count == 6:\n",
    "        #     break\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def merge_external_files(\n",
    "    f1: str,\n",
    "    f2: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Read and merge-sort two text file and output the merged file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        f1: str\n",
    "            Path to the first data file.\n",
    "\n",
    "        f2: str\n",
    "            Path to the second data file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Need a header row to keep the format correct.\n",
    "    header_row: str = ...\n",
    "    file_uid: int = random.randint(0, 9999999)\n",
    "    os.system(\n",
    "        \"rm -rf {}merged_{}.txt\".format(\n",
    "            OUTPUT_PATH,\n",
    "            file_uid,\n",
    "        )\n",
    "    )\n",
    "    Path(\n",
    "        \"{}merged_{}.txt\".format(\n",
    "            OUTPUT_PATH,\n",
    "            file_uid,\n",
    "        )\n",
    "    ).touch()\n",
    "    with open(\n",
    "        file=\"{}merged_{}.txt\".format(\n",
    "            OUTPUT_PATH,\n",
    "            file_uid,\n",
    "        ),\n",
    "        mode=\"a\",\n",
    "    ) as output_file:\n",
    "        # Implementing the \"merge\" part of the merge sort algorithm externally.\n",
    "        # Read 2 files simutenously.\n",
    "        with open(f1) as file1, open(f2) as file2:\n",
    "            # Write the head row first.\n",
    "            header_row = file1.readline()\n",
    "            next(file2)\n",
    "            output_file.write(header_row)\n",
    "\n",
    "            line1: str\n",
    "            line2: str\n",
    "            line1_written: bool = True\n",
    "            line2_written: bool = True\n",
    "            while True:\n",
    "                if line1_written:\n",
    "                    line1 = file1.readline()\n",
    "                    line1_written = False\n",
    "                if line2_written:\n",
    "                    line2 = file2.readline()\n",
    "                    line2_written = False\n",
    "                if not line1 or not line2:\n",
    "                    break\n",
    "\n",
    "                # if np.datetime64(file1[i1][4]) <= np.datetime64(file2[i2][4]):\n",
    "                time1: np.datetime64 = np.datetime64(line1.split(sep=\",\")[4])\n",
    "                time2: np.datetime64 = np.datetime64(line2.split(sep=\",\")[4])\n",
    "                if time1 <= time2:\n",
    "                    # output_string: str = \",\".join(file1[i1])\n",
    "                    output_file.write(line1)\n",
    "                    line1_written = True\n",
    "                else:\n",
    "                    # output_string: str = \",\".join(file2[i2])\n",
    "                    output_file.write(line2)\n",
    "                    line2_written = True\n",
    "\n",
    "                gc.collect()\n",
    "\n",
    "            if line1:\n",
    "                output_file.write(line1)\n",
    "                while True:\n",
    "                    line1 = file1.readline()\n",
    "                    if not line1:\n",
    "                        break\n",
    "                    output_file.write(line1)\n",
    "                    gc.collect()\n",
    "            if line2:\n",
    "                output_file.write(line2)\n",
    "                while True:\n",
    "                    line2 = file2.readline()\n",
    "                    if not line2:\n",
    "                        break\n",
    "                    output_file.write(line2)\n",
    "                    gc.collect()\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    # Remove the two files after merged.\n",
    "    os.system(command=\"rm -rf {} {}\".format(f1, f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    split_files: list[str] = os.listdir(path=OUTPUT_PATH)\n",
    "    no_of_files: int = len(split_files)\n",
    "\n",
    "    if no_of_files == 1:\n",
    "        break\n",
    "\n",
    "    if no_of_files % 2 == 0:\n",
    "        for file_index in range(0, no_of_files, 2):\n",
    "            merge_external_files(\n",
    "                f1=\"{}{}\".format(\n",
    "                    OUTPUT_PATH,\n",
    "                    split_files[file_index],\n",
    "                ),\n",
    "                f2=\"{}{}\".format(\n",
    "                    OUTPUT_PATH,\n",
    "                    split_files[file_index + 1],\n",
    "                ),\n",
    "            )\n",
    "            gc.collect()\n",
    "    else:\n",
    "        for file_index in range(0, no_of_files - 1, 2):\n",
    "            merge_external_files(\n",
    "                f1=\"{}{}\".format(\n",
    "                    OUTPUT_PATH,\n",
    "                    split_files[file_index],\n",
    "                ),\n",
    "                f2=\"{}{}\".format(\n",
    "                    OUTPUT_PATH,\n",
    "                    split_files[file_index + 1],\n",
    "                ),\n",
    "            )\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunked_df: pandas.io.parsers.readers.TextFileReader = pd.read_csv(\n",
    "    filepath_or_buffer=\"./out/merged_6565788.txt\",\n",
    "    chunksize=600,\n",
    "    sep=\",\",\n",
    ")\n",
    "\n",
    "df: pd.DataFrame\n",
    "for df in chunked_df:\n",
    "    ts: np.ndarray = df[\"timestamps\"].to_numpy()\n",
    "    is_sorted = lambda a: np.all(a[:-1] <= a[1:])\n",
    "    print(is_sorted(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
